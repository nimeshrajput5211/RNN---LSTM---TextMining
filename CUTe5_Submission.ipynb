{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>categories</th>\n",
       "      <th>converse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PRESCRIPTION</td>\n",
       "      <td>patients aware that he needs rov for refill na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>mom wants to know if the drugname needs some d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ASK_A_DOCTOR</td>\n",
       "      <td>patients to discuss drugname she says she has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>fyi nortryptline medication patient prescripti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MISCELLANEOUS</td>\n",
       "      <td>letter of patient establishment request name s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   field     categories                                           converse\n",
       "0      0   PRESCRIPTION  patients aware that he needs rov for refill na...\n",
       "1      1   ASK_A_DOCTOR  mom wants to know if the drugname needs some d...\n",
       "2      2   ASK_A_DOCTOR  patients to discuss drugname she says she has ...\n",
       "3      3  MISCELLANEOUS  fyi nortryptline medication patient prescripti...\n",
       "4      4  MISCELLANEOUS  letter of patient establishment request name s..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57244, 3)\n"
     ]
    }
   ],
   "source": [
    "train= train[~train['converse'].isnull()]\n",
    "print (train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the LabelEncoder from scikit-learn to convert text labels to integers\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "y = lbl_enc.fit_transform(train.categories.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test - train split\n",
    "#xtrain, xvalid, ytrain, yvalid = train_test_split(train.categories.values, y, \n",
    "                                 #                 stratify=y, \n",
    "                                  #                random_state=42, \n",
    "                                   #               test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train['converse'], y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45795,)\n",
      "(11449,)\n"
     ]
    }
   ],
   "source": [
    "# Inspect\n",
    "print (xtrain.shape)\n",
    "print (xvalid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [03:09, 2106.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 45795/45795 [02:39<00:00, 287.67it/s]\n",
      "100%|███████████████████████████████████| 11449/11449 [00:37<00:00, 303.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# create sentence vectors using the above function for training and validation set\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xvalid_glove = np.array(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K # This 'K' can be used to create user defined functions in keras\n",
    "\n",
    "# Define a custom function in keras to compute recall.\n",
    "# Arguments:\n",
    "# y_true - Actual labels\n",
    "# y_pred - Predicted labels\n",
    "def recall(y_true, y_pred):\n",
    "    TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    PP = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = TP / (PP + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=300, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy', recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45795 samples, validate on 11449 samples\n",
      "Epoch 1/20\n",
      " - 21s - loss: 0.4296 - acc: 0.8329 - recall: 0.8065 - val_loss: 0.5952 - val_acc: 0.7865 - val_recall: 0.7589\n",
      "Epoch 2/20\n",
      " - 21s - loss: 0.4198 - acc: 0.8366 - recall: 0.8116 - val_loss: 0.5901 - val_acc: 0.7874 - val_recall: 0.7614\n",
      "Epoch 3/20\n",
      " - 21s - loss: 0.4114 - acc: 0.8389 - recall: 0.8145 - val_loss: 0.5920 - val_acc: 0.7885 - val_recall: 0.7560\n",
      "Epoch 4/20\n",
      " - 16s - loss: 0.3997 - acc: 0.8433 - recall: 0.8191 - val_loss: 0.6054 - val_acc: 0.7882 - val_recall: 0.7616\n",
      "Epoch 5/20\n",
      " - 13s - loss: 0.3935 - acc: 0.8463 - recall: 0.8249 - val_loss: 0.6039 - val_acc: 0.7855 - val_recall: 0.7595\n",
      "Epoch 6/20\n",
      " - 10s - loss: 0.3851 - acc: 0.8493 - recall: 0.8275 - val_loss: 0.6116 - val_acc: 0.7849 - val_recall: 0.7582\n",
      "Epoch 7/20\n",
      " - 10s - loss: 0.3771 - acc: 0.8520 - recall: 0.8322 - val_loss: 0.6116 - val_acc: 0.7856 - val_recall: 0.7612\n",
      "Epoch 8/20\n",
      " - 10s - loss: 0.3720 - acc: 0.8525 - recall: 0.8330 - val_loss: 0.6197 - val_acc: 0.7864 - val_recall: 0.7613\n",
      "Epoch 9/20\n",
      " - 10s - loss: 0.3680 - acc: 0.8573 - recall: 0.8383 - val_loss: 0.6188 - val_acc: 0.7834 - val_recall: 0.7554\n",
      "Epoch 10/20\n",
      " - 10s - loss: 0.3642 - acc: 0.8563 - recall: 0.8373 - val_loss: 0.6313 - val_acc: 0.7855 - val_recall: 0.7622\n",
      "Epoch 11/20\n",
      " - 10s - loss: 0.3584 - acc: 0.8599 - recall: 0.8418 - val_loss: 0.6265 - val_acc: 0.7857 - val_recall: 0.7591\n",
      "Epoch 12/20\n",
      " - 9s - loss: 0.3430 - acc: 0.8649 - recall: 0.8475 - val_loss: 0.6455 - val_acc: 0.7843 - val_recall: 0.7628\n",
      "Epoch 13/20\n",
      " - 9s - loss: 0.3415 - acc: 0.8664 - recall: 0.8496 - val_loss: 0.6464 - val_acc: 0.7798 - val_recall: 0.7581\n",
      "Epoch 14/20\n",
      " - 9s - loss: 0.3391 - acc: 0.8673 - recall: 0.8503 - val_loss: 0.6412 - val_acc: 0.7818 - val_recall: 0.7563\n",
      "Epoch 15/20\n",
      " - 9s - loss: 0.3307 - acc: 0.8711 - recall: 0.8553 - val_loss: 0.6538 - val_acc: 0.7855 - val_recall: 0.7635\n",
      "Epoch 16/20\n",
      " - 9s - loss: 0.3298 - acc: 0.8714 - recall: 0.8563 - val_loss: 0.6478 - val_acc: 0.7819 - val_recall: 0.7611\n",
      "Epoch 17/20\n",
      " - 9s - loss: 0.3246 - acc: 0.8727 - recall: 0.8587 - val_loss: 0.6509 - val_acc: 0.7818 - val_recall: 0.7619\n",
      "Epoch 18/20\n",
      " - 9s - loss: 0.3170 - acc: 0.8752 - recall: 0.8614 - val_loss: 0.6663 - val_acc: 0.7815 - val_recall: 0.7592\n",
      "Epoch 19/20\n",
      " - 9s - loss: 0.3198 - acc: 0.8742 - recall: 0.8602 - val_loss: 0.6601 - val_acc: 0.7795 - val_recall: 0.7551\n",
      "Epoch 20/20\n",
      " - 9s - loss: 0.3112 - acc: 0.8776 - recall: 0.8640 - val_loss: 0.6864 - val_acc: 0.7794 - val_recall: 0.7596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd8a82d1b38>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=20, verbose=2, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use LSTM\n",
    "\n",
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 70\n",
    "\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "\n",
    "# zero pad the sequences\n",
    "xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 39288/39288 [00:00<00:00, 93700.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(6))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy', recall], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45795 samples, validate on 11449 samples\n",
      "Epoch 1/5\n",
      " - 197s - loss: 1.6662 - acc: 0.2598 - recall: 0.0148 - val_loss: 1.4332 - val_acc: 0.4153 - val_recall: 0.1342\n",
      "Epoch 2/5\n",
      " - 191s - loss: 1.4457 - acc: 0.3850 - recall: 0.1380 - val_loss: 1.2012 - val_acc: 0.5462 - val_recall: 0.3024\n",
      "Epoch 3/5\n",
      " - 196s - loss: 1.2909 - acc: 0.4825 - recall: 0.2467 - val_loss: 1.1450 - val_acc: 0.5788 - val_recall: 0.3002\n",
      "Epoch 4/5\n",
      " - 199s - loss: 1.1974 - acc: 0.5408 - recall: 0.2987 - val_loss: 1.0194 - val_acc: 0.6363 - val_recall: 0.3587\n",
      "Epoch 5/5\n",
      " - 201s - loss: 1.1319 - acc: 0.5842 - recall: 0.3422 - val_loss: 0.9488 - val_acc: 0.6667 - val_recall: 0.3944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xd8a8602160>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=5,  validation_data=(xvalid_pad, yvalid_enc),verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
